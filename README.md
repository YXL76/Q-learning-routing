# Q-learning-routing

## 实验目的

研究基于 Q-learning 的车联网路由算法的使用效果

## 实验内容

设计编写基于 Q-learning 的车联网路由算法程序，并比较不同的学习参数**a1**，**a2**值是如何影响**不同数量的信源**的通信效果

## 实验方法和算法原理

### 强化学习

强化学习（Reinforcement learning，RL）是机器学习中的一个领域，强调如何基于环境而行动，以取得最大化的预期利益。其灵感来源于心理学中的行为主义理论，即有机体如何在环境给予的奖励或惩罚的刺激下，逐步形成对刺激的预期，产生能获得最大利益的习惯性行为。环境通常被规范为马尔可夫决策过程（Markov decision processes，MDP），所以许多强化学习算法在这种情况下使用动态规划技巧。强化学习和标准的监督式学习之间的区别在于，它并不需要出现正确的输入/输出对，可以探索未知的领域。

### Q-learning

Q-学习是一种无模型的强化学习算法。它不需要环境模型，并且可以处理具有随机过渡和奖励的问题，而无需进行适应。

### 车联网

车联网（Vehicle-to-everything）简称 V2X，是将汽车和其他车辆或是可能影响汽车的设备所进行的通信。

### 路由算法

路由是为网络中或多个网络之间或之间的流量选择路径的过程。路由分为静态路由和动态路由，出其中静态路由由中心控制节点建立；动态路由常用算法有距离向量算法和链路状态算法。车联网中对路由算法的要求为 ① 不需要集中控制，减少控制信息开销；② 针对拓扑快速变化，自适应调整策略；③ 减少端到端时延，实现网络中负载均衡；④ 提供无环路由，增强目的驱动力，减少绕路；⑤ 对终端计算能力要求不强

## 实验平台

- `OS`: `Manjaro Linux x86_64`
- `Kernel`: `5.9.16-1-MANJARO`
- `Python`: `3.9.1`
- `Matplotlib`: `3.3.3`
- `Numpy`: `1.19.4`

## 实验步骤

1. 运行`RLrouting.py`
2. 修改`RLrouting.py`中的`a1`、`a2`，重复运行代码
3. 将结果都保存在`results`文件夹中，命名方式为`[a1]-[a2].png`

## 实验总结

1. 一般来说，平均时延随着源节点数量的增加而增大
2. 当`a1=0.5`时，`a2=0.5`比`a2=0.8`的通信效果好
3. 当`a1=0.8`时，对于源节点数量为3和9时，`a2=0.5`比`a2=0.8`的通信效果好；对于源节点数量为5和7时，`a2=0.5`比`a2=0.8`的通信效果差
